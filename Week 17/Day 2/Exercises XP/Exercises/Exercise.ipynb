{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of the 'chipo' dataset:\n",
      "  order_id\\tquantity\\titem_name\\tchoice_description\\titem_price\n",
      "0   1\\t1\\tChips and Fresh Tomato Salsa\\tNULL\\t$2.39            \n",
      "1                   1\\t1\\tIzze\\t[Clementine]\\t$3.39            \n",
      "2            1\\t1\\tNantucket Nectar\\t[Apple]\\t$3.39            \n",
      "3  1\\t1\\tChips and Tomatillo-Green Chili Salsa\\tN...           \n",
      "4                  3\\t1\\tSide of Chips\\tNULL\\t$1.69            \n",
      "5            5\\t1\\tChips and Guacamole\\tNULL\\t$4.45            \n",
      "6            7\\t1\\tChips and Guacamole\\tNULL\\t$4.45            \n",
      "7  8\\t1\\tChips and Tomatillo-Green Chili Salsa\\tN...           \n",
      "8                9\\t2\\tCanned Soda\\t[Sprite]\\t$2.18            \n",
      "9           10\\t1\\tChips and Guacamole\\tNULL\\t$4.45            \n",
      "\n",
      "Total number of entries (rows) in 'chipo':\n",
      "1795\n",
      "\n",
      "Total number of columns in 'chipo':\n",
      "1\n",
      "\n",
      "Column names in 'chipo':\n",
      "Index(['order_id\\tquantity\\titem_name\\tchoice_description\\titem_price'], dtype='object')\n",
      "\n",
      "Index of 'chipo':\n",
      "RangeIndex(start=0, stop=1795, step=1)\n",
      "\n",
      "Column names in 'chipo':\n",
      "Index(['order_id\\tquantity\\titem_name\\tchoice_description\\titem_price'], dtype='object')\n",
      "\n",
      "'item_name' column not found in the dataset.\n",
      "\n",
      "'quantity' column not found in the dataset.\n",
      "\n",
      "'choice_description' column not found in the dataset.\n",
      "\n",
      "'item_price' column not found in the dataset.\n",
      "\n",
      "'order_id' column not found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# ðŸŒŸ Exercise 1: Getting & Knowing Your Data (chipo)\n",
    "\n",
    "# 1. Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 2. Retrieve the dataset and assign it to a variable 'chipo'\n",
    "# Adding error handling and specifying potential fixes like on_bad_lines and encoding\n",
    "try:\n",
    "    chipo = pd.read_csv('data.csv', on_bad_lines='skip', encoding='utf-8')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the CSV file: {e}\")\n",
    "\n",
    "# 3. Display the first 10 rows\n",
    "print(\"First 10 rows of the 'chipo' dataset:\")\n",
    "print(chipo.head(10))\n",
    "\n",
    "# 4. Determine the total number of entries (rows)\n",
    "print(\"\\nTotal number of entries (rows) in 'chipo':\")\n",
    "print(chipo.shape[0])\n",
    "\n",
    "# 5. Find the total number of columns in 'chipo'\n",
    "print(\"\\nTotal number of columns in 'chipo':\")\n",
    "print(chipo.shape[1])\n",
    "\n",
    "# 6. Print all column names\n",
    "print(\"\\nColumn names in 'chipo':\")\n",
    "print(chipo.columns)\n",
    "\n",
    "# 7. Understand how the DataFrame is indexed\n",
    "print(\"\\nIndex of 'chipo':\")\n",
    "print(chipo.index)\n",
    "\n",
    "# Check the column names\n",
    "print(\"\\nColumn names in 'chipo':\")\n",
    "print(chipo.columns)\n",
    "\n",
    "# 8. Check if 'item_name' column exists\n",
    "if 'item_name' in chipo.columns:\n",
    "    # Find the most ordered item in 'item_name'\n",
    "    print(\"\\nMost ordered item:\")\n",
    "    print(chipo['item_name'].value_counts().head(1))\n",
    "else:\n",
    "    print(\"\\n'item_name' column not found in the dataset.\")\n",
    "\n",
    "# 9. Find the total number of items ordered\n",
    "# Check if 'quantity' column exists\n",
    "if 'quantity' in chipo.columns:\n",
    "    print(\"\\nTotal number of items ordered:\")\n",
    "    print(chipo['quantity'].sum())\n",
    "else:\n",
    "    print(\"\\n'quantity' column not found in the dataset.\")\n",
    "\n",
    "# 10. Find the most ordered item from 'choice_description'\n",
    "# Check if 'choice_description' column exists\n",
    "if 'choice_description' in chipo.columns:\n",
    "    print(\"\\nMost ordered item from 'choice_description':\")\n",
    "    print(chipo['choice_description'].value_counts().head(1))\n",
    "else:\n",
    "    print(\"\\n'choice_description' column not found in the dataset.\")\n",
    "\n",
    "# 11. Convert 'item_price' to float datatype using apply and lambda\n",
    "# Removing the '$' and converting to float\n",
    "if 'item_price' in chipo.columns:\n",
    "    chipo['item_price'] = chipo['item_price'].apply(lambda x: float(str(x).replace('$', '')))\n",
    "else:\n",
    "    print(\"\\n'item_price' column not found in the dataset.\")\n",
    "\n",
    "# 12. Calculate the total revenue for the dataset\n",
    "if 'item_price' in chipo.columns:\n",
    "    print(\"\\nTotal revenue:\")\n",
    "    print(chipo['item_price'].sum())\n",
    "\n",
    "# 13. Find the total number of orders\n",
    "if 'order_id' in chipo.columns:\n",
    "    print(\"\\nTotal number of unique orders:\")\n",
    "    print(chipo['order_id'].nunique())\n",
    "else:\n",
    "    print(\"\\n'order_id' column not found in the dataset.\")\n",
    "\n",
    "# 14. Compute the average order value\n",
    "if 'item_price' in chipo.columns and 'order_id' in chipo.columns:\n",
    "    total_revenue = chipo['item_price'].sum()\n",
    "    total_orders = chipo['order_id'].nunique()\n",
    "    average_order_value = total_revenue / total_orders\n",
    "    print(\"\\nAverage order value:\")\n",
    "    print(average_order_value)\n",
    "\n",
    "# 15. Determine the total number of unique items sold\n",
    "if 'item_name' in chipo.columns:\n",
    "    print(\"\\nTotal number of unique items sold:\")\n",
    "    print(chipo['item_name'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in 'chipo':\n",
      "Index(['order_id\\tquantity\\titem_name\\tchoice_description\\titem_price'], dtype='object')\n",
      "\n",
      "'quantity' column not found in the dataset.\n",
      "\n",
      "'item_price' column not found in the dataset.\n",
      "\n",
      "'item_name' column not found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# ðŸŒŸ Exercise 2: Filtering & Sorting (chipo)\n",
    "\n",
    "# Use the already created dataframe â€˜chipoâ€™ from the previous section\n",
    "\n",
    "# 1. Check the column names to identify any discrepancies\n",
    "print(\"Column names in 'chipo':\")\n",
    "print(chipo.columns)\n",
    "\n",
    "# 2. Filter data for a specific condition (e.g., items with quantity greater than 10)\n",
    "# Check if 'quantity' column exists\n",
    "if 'quantity' in chipo.columns:\n",
    "    filtered_data = chipo[chipo['quantity'] > 10]\n",
    "    print(\"\\nFiltered data (quantity > 10):\")\n",
    "    print(filtered_data.head())\n",
    "else:\n",
    "    print(\"\\n'quantity' column not found in the dataset.\")\n",
    "\n",
    "# 3. Sort the data by a specific column (e.g., 'item_price')\n",
    "# Check if 'item_price' column exists\n",
    "if 'item_price' in chipo.columns:\n",
    "    sorted_data = chipo.sort_values(by='item_price', ascending=False)\n",
    "    print(\"\\nData sorted by 'item_price':\")\n",
    "    print(sorted_data.head())\n",
    "else:\n",
    "    print(\"\\n'item_price' column not found in the dataset.\")\n",
    "\n",
    "# 4. Filter data for a specific item (e.g., 'Chicken Bowl')\n",
    "# Check if 'item_name' column exists\n",
    "if 'item_name' in chipo.columns:\n",
    "    chicken_bowl_data = chipo[chipo['item_name'] == 'Chicken Bowl']\n",
    "    print(\"\\nData for 'Chicken Bowl':\")\n",
    "    print(chicken_bowl_data.head())\n",
    "else:\n",
    "    print(\"\\n'item_name' column not found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of the 'users' dataset:\n",
      "  user_id|age|gender|occupation|zip_code\n",
      "0                1|24|M|technician|85711\n",
      "1                     2|53|F|other|94043\n",
      "2                    3|23|M|writer|32067\n",
      "3                4|24|M|technician|43537\n",
      "4                     5|33|F|other|15213\n",
      "\n",
      "Column names in 'users':\n",
      "Index(['user_id|age|gender|occupation|zip_code'], dtype='object')\n",
      "\n",
      "'occupation' or 'age' column not found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# 2. Retrieve the dataset and assign it to a variable 'users'\n",
    "file_path = 'users.csv'  # Update this path to the correct location of your CSV file\n",
    "try:\n",
    "    users = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error loading the CSV file: [Errno 2] No such file or directory: '{file_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# 3. Check if the 'users' DataFrame was loaded successfully\n",
    "if 'users' in locals():\n",
    "    # 4. Display the first few rows to understand the structure\n",
    "    print(\"\\nFirst few rows of the 'users' dataset:\")\n",
    "    print(users.head())\n",
    "\n",
    "    # 5. Print the column names to ensure they are as expected\n",
    "    print(\"\\nColumn names in 'users':\")\n",
    "    print(users.columns)\n",
    "\n",
    "    # 6. Calculate the mean age per occupation\n",
    "    if 'occupation' in users.columns and 'age' in users.columns:\n",
    "        mean_age_per_occupation = users.groupby('occupation')['age'].mean()\n",
    "        print(\"\\nMean age per occupation:\")\n",
    "        print(mean_age_per_occupation)\n",
    "    else:\n",
    "        print(\"\\n'occupation' or 'age' column not found in the dataset.\")\n",
    "else:\n",
    "    print(\"\\nThe 'users' DataFrame was not defined due to an error in loading the CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged along rows (all_data):\n",
      "   ID     Name\n",
      "0   1    Alice\n",
      "1   2      Bob\n",
      "2   3  Charlie\n",
      "3   4    David\n",
      "4   5      Eve\n",
      "5   6    Frank\n",
      "6   7    Grace\n",
      "7   8    Heidi\n",
      "\n",
      "Merged along columns (all_data_col):\n",
      "   ID     Name  ID   Name\n",
      "0   1    Alice   5    Eve\n",
      "1   2      Bob   6  Frank\n",
      "2   3  Charlie   7  Grace\n",
      "3   4    David   8  Heidi\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrames\n",
    "data1 = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David']\n",
    "})\n",
    "\n",
    "data2 = pd.DataFrame({\n",
    "    'ID': [5, 6, 7, 8],\n",
    "    'Name': ['Eve', 'Frank', 'Grace', 'Heidi']\n",
    "})\n",
    "\n",
    "data3 = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'Score': [85, 92, 78, 88, 90, 79, 94, 84]\n",
    "})\n",
    "\n",
    "# Merge data1 and data2 along rows\n",
    "all_data = pd.concat([data1, data2], axis=0, ignore_index=True)\n",
    "\n",
    "# Merge data1 and data2 along columns\n",
    "all_data_col = pd.concat([data1, data2], axis=1)\n",
    "\n",
    "# Display the results\n",
    "print(\"Merged along rows (all_data):\")\n",
    "print(all_data)\n",
    "\n",
    "print(\"\\nMerged along columns (all_data_col):\")\n",
    "print(all_data_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in 'iris':\n",
      "sepal_length    0\n",
      "sepal_width     0\n",
      "petal_length    0\n",
      "petal_width     0\n",
      "species         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ðŸŒŸ Exercise 5: Deleting (iris)\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Retrieve the dataset and assign it to â€˜irisâ€™\n",
    "try:\n",
    "    iris = pd.read_csv('iris.csv', on_bad_lines='skip', encoding='utf-8')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the CSV file: {e}\")\n",
    "\n",
    "# Assign appropriate column names\n",
    "iris.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "# Check if there are any missing values\n",
    "print(\"\\nMissing values in 'iris':\")\n",
    "print(iris.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rearranged DataFrame 'pokemon':\n",
      "         name   type  hp   evolution pokedex\n",
      "0   Bulbasaur  grass  45     Ivysaur     yes\n",
      "1  Charmander   fire  39  Charmeleon      no\n",
      "2    Squirtle  water  44   Wartortle     yes\n",
      "3    Caterpie    bug  45     Metapod      no\n"
     ]
    }
   ],
   "source": [
    "# ðŸŒŸ Exercise 6: Creating Series and DataFrames (pokemon)\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Create a data dictionary and convert it into a DataFrame\n",
    "pokemon_data = {\n",
    "    'evolution': ['Ivysaur', 'Charmeleon', 'Wartortle', 'Metapod'],\n",
    "    'hp': [45, 39, 44, 45],\n",
    "    'name': ['Bulbasaur', 'Charmander', 'Squirtle', 'Caterpie'],\n",
    "    'pokedex': ['yes', 'no', 'yes', 'no'],\n",
    "    'type': ['grass', 'fire', 'water', 'bug']\n",
    "}\n",
    "pokemon = pd.DataFrame(pokemon_data)\n",
    "\n",
    "# Rearrange the columns in the order: â€˜nameâ€™, â€˜typeâ€™, â€˜hpâ€™, â€˜evolutionâ€™, â€˜pokedexâ€™\n",
    "pokemon = pokemon[['name', 'type', 'hp', 'evolution', 'pokedex']]\n",
    "print(\"\\nRearranged DataFrame 'pokemon':\")\n",
    "print(pokemon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 entries of 'baby_names':\n",
      "   Unnamed: 0     Id      Name    Year Gender State  Count\n",
      "0       11349  11350      Emma  2004.0      F    AK   62.0\n",
      "1       11350  11351   Madison  2004.0      F    AK   48.0\n",
      "2       11351  11352    Hannah  2004.0      F    AK   46.0\n",
      "3       11352  11353     Grace  2004.0      F    AK   44.0\n",
      "4       11353  11354     Emily  2004.0      F    AK   41.0\n",
      "5       11354  11355   Abigail  2004.0      F    AK   37.0\n",
      "6       11355  11356    Olivia  2004.0      F    AK   33.0\n",
      "7       11356  11357  Isabella  2004.0      F    AK   30.0\n",
      "8       11357  11358    Alyssa  2004.0      F    AK   29.0\n",
      "9       11358  11359    Sophia  2004.0      F    AK   28.0\n",
      "\n",
      "DataFrame 'baby_names' after dropping columns:\n",
      "       Name    Year Gender State  Count\n",
      "0      Emma  2004.0      F    AK   62.0\n",
      "1   Madison  2004.0      F    AK   48.0\n",
      "2    Hannah  2004.0      F    AK   46.0\n",
      "3     Grace  2004.0      F    AK   44.0\n",
      "4     Emily  2004.0      F    AK   41.0\n",
      "5   Abigail  2004.0      F    AK   37.0\n",
      "6    Olivia  2004.0      F    AK   33.0\n",
      "7  Isabella  2004.0      F    AK   30.0\n",
      "8    Alyssa  2004.0      F    AK   29.0\n",
      "9    Sophia  2004.0      F    AK   28.0\n"
     ]
    }
   ],
   "source": [
    "# ðŸŒŸ Exercise 7: Descriptive Statistics (baby_names)\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Download and load the US Baby Names data into a DataFrame named baby_names\n",
    "try:\n",
    "    baby_names = pd.read_csv('baby_names.csv', on_bad_lines='skip', encoding='utf-8')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the CSV file: {e}\")\n",
    "\n",
    "# Display the first 10 entries\n",
    "print(\"\\nFirst 10 entries of 'baby_names':\")\n",
    "print(baby_names.head(10))\n",
    "\n",
    "# Delete the columns â€˜Unnamed: 0â€™ and â€˜Idâ€™\n",
    "baby_names = baby_names.drop(columns=['Unnamed: 0', 'Id'], errors='ignore')\n",
    "print(\"\\nDataFrame 'baby_names' after dropping columns:\")\n",
    "print(baby_names.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The frequency of the dataset is: D\n",
      "\n",
      "DataFrame 'weather_data' with 'Date' as index:\n",
      "            Temperature  Humidity  Wind_Speed\n",
      "Date                                         \n",
      "2021-01-01           30        80           5\n",
      "2021-01-02           31        75           6\n",
      "2021-01-03           29        70           7\n",
      "2021-01-04           32        85           8\n",
      "2021-01-05           33        90           9\n",
      "2021-01-06           34        80           4\n",
      "2021-01-07           28        75          10\n",
      "2021-01-08           27        60           5\n",
      "2021-01-09           25        65           6\n",
      "2021-01-10           35        70           7\n"
     ]
    }
   ],
   "source": [
    "# ðŸŒŸ Exercise 8: Handling Time Series Data (investor_data)\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Create dataset using pd.date_range and specified columns\n",
    "date_rng = pd.date_range(start='2021-01-01', end='2021-01-10', freq='D')\n",
    "temperature = [30, 31, 29, 32, 33, 34, 28, 27, 25, 35]\n",
    "humidity = [80, 75, 70, 85, 90, 80, 75, 60, 65, 70]\n",
    "wind_speed = [5, 6, 7, 8, 9, 4, 10, 5, 6, 7]\n",
    "\n",
    "weather_data = pd.DataFrame({\n",
    "    'Date': date_rng,\n",
    "    'Temperature': temperature,\n",
    "    'Humidity': humidity,\n",
    "    'Wind_Speed': wind_speed\n",
    "})\n",
    "\n",
    "# Determine the frequency of the dataset\n",
    "freq = pd.infer_freq(weather_data['Date'])\n",
    "print(f\"\\nThe frequency of the dataset is: {freq}\")\n",
    "\n",
    "# Set â€˜Dateâ€™ as the index of the DataFrame\n",
    "weather_data.set_index('Date', inplace=True)\n",
    "print(\"\\nDataFrame 'weather_data' with 'Date' as index:\")\n",
    "print(weather_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
